{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Öffne die JSON-Datei und lade den Inhalt\n",
    "with open('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/api_token.json', 'r') as api_file:\n",
    "    api_token_file = json.load(api_file)\n",
    "\n",
    "# Extrahiere die Variable aus den Daten\n",
    "api_token = api_token_file['Hugging_face_token']\n",
    "open_ai_token = api_token_file['Open_api_token']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory,ConversationSummaryMemory\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token=api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Where is Langchain?'\n",
    "output = llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your API Key from OpenAI\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Create the conversation history and add the first AI message\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hello! Ask me anything about Python programming!\")\n",
    "\n",
    "# Add the user message to the history and call the model\n",
    "history.add_user_message(\"What is a list comprehension?\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)\n",
    "\n",
    "# Add another user message and call the model\n",
    "history.add_user_message(\"Describe the same in fewer words\")\n",
    "\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Define a buffer memory\n",
    "memory = ConversationBufferMemory(size=2)\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "buffer_chain.predict(input=\"Explain what is seaborn in python.\")\n",
    "buffer_chain.predict(input=\"Tell me about Germany something\")\n",
    "buffer_chain.predict(input=\"Tell me about Xbox something.\")\n",
    "buffer_chain.predict(input=\"Tell me about Playstation something.\")\n",
    "buffer_chain.predict(input=\"What was my first question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Define a summary memory that uses an OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=open_ai_token))\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "summary_chain.predict(input=\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
    "summary_chain.predict(input=\"Use an analogy to describe it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Retrieval Augmented Generation (RAG)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf\")\n",
    "data = loader.load()\n",
    "print(data[36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third party document loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import HNLoader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/\")\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting by Charakter**\n",
    "\n",
    "Dieser Code erstellt eine Instanz der CharacterTextSplitter-Klasse und verwendet sie, um einen gegebenen Text in Chunks aufzuteilen.\n",
    "\n",
    "Zuerst wird eine Zeichenkette quote definiert, die den Text enthält, der aufgeteilt werden soll. Dann werden die Variablen chunk_size und chunk_overlap festgelegt, um die Größe der Chunks und die Überlappung zwischen den Chunks anzugeben.\n",
    "\n",
    "Anschließend wird eine Instanz der CharacterTextSplitter-Klasse erstellt und den Variablen chunk_size und chunk_overlap übergeben. Diese Klasse implementiert die Logik zum Aufteilen des Textes in Chunks.\n",
    "\n",
    "Schließlich wird die Methode split_text() der splitter-Instanz aufgerufen, um den Text in Chunks aufzuteilen. Das Ergebnis wird in der Variable docs gespeichert. Schließlich werden die Chunks mit print(docs) ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote) \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_overlap = 10,\n",
    "    chunk_size=24\n",
    "\n",
    ")\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader(\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Define variables\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Split the HTML\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.'])\n",
    "\n",
    "docs = splitter.split_documents(data)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = open_ai_token\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf')\n",
    "data = loader.load()\n",
    "print(data[89]) ## seite 74\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Split the quote using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "docs = splitter.split_documents(data) \n",
    "docs\n",
    "# Define an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=open_ai_token)\n",
    "\n",
    "# Create the Chroma vector DB using the OpenAI embedding function; persist the database\n",
    "vectordb = Chroma(\n",
    "    persist_directory='embedding/chroma/',\n",
    "    embedding_function=embedding_model)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/docs_for_llm_pipline/merged.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=open_ai_token)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=open_ai_token),         chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the chain on the query provided\n",
    "query = \"Which sources do you use?\"\n",
    "print(qa.invoke(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf')\n",
    "data = loader.load()\n",
    "data\n",
    "print(data[1])\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=100,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=open_ai_token)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the function for the question to be answered with\n",
    "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, \n",
    "           openai_api_key=open_ai_token), chain_type=\"stuff\", \n",
    "           retriever=docstorage.as_retriever()\n",
    ")\n",
    "\n",
    "# Run the query on the documents\n",
    "results = qa({\"question\": \"can you give me the key takeways of the chapter '3.1 Marketing mix modeling Regression Model' ?\"}, return_only_outputs=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import HuggingFaceHub\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Setze die Repository-ID für Mistral-7B-Instruct\n",
    "repo_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "# Erstelle eine Instanz von HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=api_token)\n",
    "\n",
    "# Lade nur die Seite 5 des PDF-Dokuments\n",
    "loader = PyPDFLoader(\"/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf\")\n",
    "page_number = 5\n",
    "data = loader.load_page(page_number)\n",
    "\n",
    "# Erstelle eine Instanz von HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='bert-base-uncased')  # Beispiel-Modellname, anpassen nach Bedarf\n",
    "\n",
    "# Generiere die Einbettungen für die geladene Seite\n",
    "embeddings = embedding_model.embed_text(data)\n",
    "\n",
    "\n",
    "# # Definiere die Größe der Abschnitte für den Splitter\n",
    "# chunk_size = 200\n",
    "# chunk_overlap = 50\n",
    "\n",
    "# # Teile den Text mit RecursiveCharacterTextSplitter\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=chunk_size,\n",
    "#     chunk_overlap=chunk_overlap)\n",
    "# docs = splitter.split_documents(data) \n",
    "\n",
    "# # Erstelle eine Instanz von HuggingFaceEmbeddings\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name='bert-base-uncased')  # Beispiel-Modellname, anpassen nach Bedarf\n",
    "\n",
    "# # Verwende die Embedding-Methode von HuggingFaceEmbeddings\n",
    "# embeddings = embedding_model.embed_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HIEERRRRRRR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m HIEERRRRRRR\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HIEERRRRRRR' is not defined"
     ]
    }
   ],
   "source": [
    "HIEERRRRRRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter\n",
    "\n",
    "- Unterteil ein Text in 300 Chunks. Ein Chunk ist ein Charakter eines Wortes das heißt es werden 300 Zeichen aus der Textpassage genommen.\n",
    "- Weiterhin setzten wir die Größe des Overlaps auf 100 Zeichen. Das heißt das von einem Chunk zum anderen Chunk 100 Zeichen überlappen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50, \n",
    "    length_function = len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **filepath = '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf':** Hier wird der Pfad zur PDF-Datei festgelegt, die gelesen werden soll.\n",
    "\n",
    "- **loader = PyPDFLoader(filepath):** Hier wird eine Instanz der PyPDFLoader-Klasse erstellt, die zum Laden von PDF-Dateien verwendet wird. Der Pfad zur PDF-Datei wird als Argument an den Konstruktor der Klasse übergeben.\n",
    "\n",
    "- **chunks = loader.load_and_split(text_splitter=text_splitter):** Die Methode load_and_split der PyPDFLoader-Instanz wird aufgerufen, um die PDF-Datei zu laden und ihren Inhalt in Chunks aufzuteilen. Der text_splitter, der zuvor als Instanz der RecursiveCharacterTextSplitter-Klasse erstellt wurde, wird als Argument an die Methode übergeben und bestimmt, wie der Text in Chunks aufgeteilt wird. Die aufgeteilten Chunks werden in der Variable chunks gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf'\n",
    "loader = PyPDFLoader(filepath)\n",
    "chunks = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Display Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content: \n",
      " A Closer Look at Memorization in Deep Networks\n",
      "Devansh Arpit* 1 2Stanisław Jastrz˛ ebski* 3Nicolas Ballas* 1 2David Krueger* 1 2Emmanuel Bengio4\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " Maxinder S. Kanwal5Tegan Maharaj1 6Asja Fischer7Aaron Courville1 2 8Yoshua Bengio1 2 9\n",
      "Simon Lacoste-Julien1 2\n",
      "Abstract\n",
      "We examine the role of memorization in deep\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " We examine the role of memorization in deep\n",
      "learning, drawing connections to capacity, gen-\n",
      "eralization, and adversarial robustness. While\n",
      "deep networks are capable of memorizing noise\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " deep networks are capable of memorizing noise\n",
      "data, our results suggest that they tend to pri-\n",
      "oritize learning simple patterns ﬁrst. In our\n",
      "experiments, we expose qualitative differences\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " experiments, we expose qualitative differences\n",
      "in gradient-based optimization of deep neural\n",
      "networks (DNNs) on noise vs. real data. We\n",
      "also demonstrate that for appropriately tuned\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " also demonstrate that for appropriately tuned\n",
      "explicit regularization (e.g., dropout) we can\n",
      "degrade DNN training performance on noise\n",
      "datasets without compromising generalization on\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " datasets without compromising generalization on\n",
      "real data. Our analysis suggests that the notions\n",
      "of effective capacity which are dataset indepen-\n",
      "dent are unlikely to explain the generalization\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " dent are unlikely to explain the generalization\n",
      "performance of deep networks when trained with\n",
      "gradient based methods because training data it-\n",
      "self plays an important role in determining the\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " self plays an important role in determining the\n",
      "degree of memorization.\n",
      "1. Introduction\n",
      "The traditional view of generalization holds that a model\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " with sufﬁcient capacity (e.g. more parameters than training\n",
      "examples) will be able to “memorize” each example, over-\n",
      "ﬁtting the training set and yielding poor generalization to\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " validation and test sets (Goodfellow et al., 2016). Yet deep\n",
      "neural networks (DNNs) often achieve excellent gener-\n",
      "alization performance with massively over-parameterized\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " models. This phenomenon is not well-understood.\n",
      "*Equal contribution1Montréal Institute for Learning Algo-\n",
      "rithms, Canada2Université de Montréal, Canada3Jagiellonian\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " University, Krakow, Poland4McGill University, Canada\n",
      "5University of California, Berkeley, USA6Polytechnique\n",
      "Montréal, Canada7University of Bonn, Bonn, Germany\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " 8CIFAR Fellow9CIFAR Senior Fellow. Correspondence to:\n",
      "<david.krueger@umontreal.ca>.\n",
      "Proceedings of the 34thInternational Conference on Machine\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " Learning , Sydney, Australia, PMLR 70, 2017. Copyright 2017\n",
      "by the author(s).From a representation learning perspective, the general-\n",
      "ization capabilities of DNNs are believed to stem from\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " their incorporation of good generic priors (see, e.g., Ben-\n",
      "gio et al. (2009)). Lin & Tegmark (2016) further suggest\n",
      "that the priors of deep learning are well suited to the phys-\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " ical world. But while the priors of deep learning may help\n",
      "explain why DNNs learn to efﬁciently represent complex\n",
      "real-world functions, they are not restrictive enough to rule\n",
      "out memorization.\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " out memorization.\n",
      "On the contrary, deep nets are known to be universal ap-\n",
      "proximators, capable of representing arbitrarily complex\n",
      "functions given sufﬁcient capacity (Cybenko, 1989; Hornik\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " et al., 1989). Furthermore, recent work has shown that the\n",
      "expressiveness of DNNs grows exponentially with depth\n",
      "(Montufar et al., 2014; Poole et al., 2016). These works,\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " however, only examine the representational capacity , that\n",
      "is, the set of hypotheses a model is capable of expressing\n",
      "via some value of its parameters.\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " via some value of its parameters.\n",
      "Because DNN optimization is not well-understood, it is un-\n",
      "clear which of these hypotheses can actually be reached by\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " gradient-based training (Bottou, 1998). In this sense, opti-\n",
      "mization and generalization are entwined in DNNs. To ac-\n",
      "count for this, we formalize a notion of the effective capac-\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " ity (EC) of a learning algorithm A(deﬁned by specifying\n",
      "both the model and the training procedure , e.g.,“train the\n",
      "LeNet architecture (LeCun et al., 1998) for 100 epochs us-\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " ing stochastic gradient descent (SGD) with a learning rate\n",
      "of0.01”) as the set of hypotheses which can be reached\n",
      "by applying that learning algorithm on some dataset. For-\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " mally, using set-builder notation:\n",
      "EC(A) ={h|∃D such thath∈A(D)},\n",
      "whereA(D)represents the set of hypotheses that is reach-\n",
      "able byAon a datasetD1.\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " able byAon a datasetD1.\n",
      "One might suspect that DNNs effective capacity is sufﬁ-\n",
      "ciently limited by gradient-based training and early stop-\n",
      "ping to resolve the apparent paradox between DNNs’ excel-\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " lent generalization and their high representational capacity.\n",
      "However, the experiments of Zhang et al. (2017) suggest\n",
      "that this is not the case. They demonstrate that DNNs are\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " 1SinceAcan be stochastic, A(D)is a set.\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 0}\n",
      "----------------------------\n",
      "Page content: \n",
      " A Closer Look at Memorization in Deep Networks\n",
      "able to ﬁt pure noise without even needing substantially\n",
      "longer training time. Thus even the effective capacity of\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf', 'page': 1}\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks[:29]:\n",
    "    print(\"Page content: \\n\", chunk.page_content),\n",
    "    print(\"Page_metadata: \\n\", chunk.metadata),\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB \n",
    "## 1. load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Use an open source embedding model to embed the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In dieser Zeile wird eine Instanz der Klasse SentenceTransformerEmbeddings erstellt. Dies ist ein Modell, das Sätze in Vektoren umwandelt, die ihre semantische Bedeutung repräsentieren. Diese Vektoren werden oft als \"Einbettungen\" oder \"Embeddings\" bezeichnet.\n",
    "\n",
    "- Die SentenceTransformerEmbeddings-Klasse verwendet die SentenceTransformer-Bibliothek, um Einbettungen für Texte zu generieren. Die Klasse verwendet ein vortrainiertes Modell, um die Einbettungen zu generieren. Die Einbettungen können dann in einer Chroma-Datenbank gespeichert und für die spätere Verwendung abgerufen werden.\n",
    "\n",
    "- Der Parameter model_name=\"all-MiniLM-L6-v2\" gibt an, welches vortrainierte Modell verwendet werden soll. In diesem Fall wird das Modell \"all-MiniLM-L6-v2\" verwendet, das ein kleines, aber leistungsfähiges Modell für die Erzeugung von Satzeinbettungen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riccardo/anaconda3/envs/llm_train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05895749852061272, 0.05387335643172264, 0.008316715247929096, 0.0206805057823658, 0.0022009313106536865, -0.034434445202350616, 0.08473188430070877, 0.1077682226896286, 0.058957234025001526, -0.016638653352856636, 0.06211813539266586, -0.0740492045879364, -0.0303032286465168, 0.03807591274380684, -0.004251338075846434, -0.007986415177583694, 0.0031452886760234833, -0.07220274955034256, -0.01501194667071104, -0.07136189937591553, -0.08817413449287415, 0.0452018640935421, 0.009071135893464088, 0.041956428438425064, -0.0012928870273754, -0.014840386807918549, -0.03942627087235451, 0.007077477872371674, -0.036413680762052536, -0.042899180203676224, -0.04982227832078934, 0.009418361820280552, -0.022699205204844475, 0.010238043032586575, -0.00445396825671196, -0.07801611721515656, -0.070695661008358, -0.005806431174278259, 0.03875807300209999, 0.01204211637377739, -0.05325385555624962, -0.0963793396949768, -0.028097203001379967, -0.009883742779493332, 0.06731418520212173, 0.004278208594769239, 0.02124866470694542, -0.022914476692676544, -0.06625277549028397, -0.02348857931792736, 4.158759111305699e-05, 0.00964048970490694, -0.03294288367033005, 0.014496402814984322, 0.018593674525618553, 0.047485243529081345, -0.012361058965325356, 0.03878827765583992, 0.02369307167828083, -0.06135307252407074, -0.002361031249165535, -0.03253591060638428, -0.11364953964948654, 0.06236247345805168, 0.059831637889146805, 0.08645803481340408, -0.022679559886455536, 0.03328515589237213, 0.016969120129942894, -0.04824632778763771, -0.010130156762897968, -0.0021239817142486572, -0.06585216522216797, -0.04025018960237503, -0.021393608301877975, -0.02657456323504448, 0.046205248683691025, -0.044886842370033264, 0.034001607447862625, 0.05156944692134857, -0.006135378964245319, 0.001145144458860159, -0.09618978202342987, 0.029961099848151207, -0.007510695140808821, 0.015603425912559032, -0.042808204889297485, 0.043429456651210785, -0.019901875406503677, 0.07665636390447617, -0.01384327095001936, 0.034392017871141434, 0.07292252033948898, 0.0133975213393569, -0.07558221369981766, -0.03218085318803787, -0.05393291264772415, -0.016610195860266685, -0.10869736224412918, 0.23937205970287323, -0.020459236577153206, 0.014236437156796455, 0.02164141833782196, 0.09602873772382736, -0.010307742282748222, 0.01200162898749113, -0.018780171871185303, -0.04145337641239166, 0.048597563058137894, -0.005922127515077591, -0.06287381052970886, -0.03733189404010773, 0.013004820793867111, -0.009868098422884941, 0.048202723264694214, -0.02079993113875389, -0.08003193140029907, 0.005167827010154724, 0.00725804315879941, -0.014367008581757545, 0.06591780483722687, 0.04329393804073334, -0.06329438835382462, 0.014587262645363808, -0.003063279204070568, -0.03614052012562752, 0.03620026260614395, -4.612938114318177e-33, 0.0409914031624794, 0.004939346108585596, -0.006083912216126919, 0.043096765875816345, 0.0167239923030138, 0.09288540482521057, -0.03938951715826988, 0.004293610341846943, -0.05639263615012169, 0.0672842413187027, -0.16096237301826477, -0.0779045820236206, -0.03543432056903839, 0.0018369597382843494, 0.06707488000392914, 0.028946315869688988, 0.03300480917096138, 0.029375018551945686, -0.0051882704719901085, -0.027864258736371994, -0.01788497157394886, 0.1109727993607521, -0.04470197111368179, 0.04207243770360947, 0.011976080946624279, -0.08128233253955841, 0.008002789691090584, -0.07154875248670578, -0.06217927113175392, 0.033613819628953934, -0.01154992263764143, 0.027502810582518578, -0.07996906340122223, 0.002480847528204322, 0.02366090752184391, -0.09160154312849045, 0.06299559026956558, -0.004837492015212774, -0.0030987428035587072, -0.024675210937857628, 0.08662998676300049, 0.024653417989611626, -9.652456355979666e-05, -0.03318057581782341, -0.009754078462719917, 0.08528172969818115, 0.09049248695373535, -0.005956639070063829, 0.011446849443018436, 0.042310360819101334, 0.0035262685269117355, -0.01033497042953968, -0.07504887878894806, -0.012202832847833633, 0.03234664723277092, 0.025926638394594193, 0.019049691036343575, -0.012024185620248318, 0.05401187390089035, 0.01013104896992445, 0.04285314679145813, -0.040263108909130096, 0.013790000230073929, -0.06209595873951912, -0.009781830944120884, -0.033753663301467896, -0.02063378319144249, -0.03565194085240364, 0.05844387784600258, -0.0428612157702446, -0.05484124273061752, 0.0035845881793648005, 0.0965285524725914, -0.039651673287153244, -0.008388887159526348, -0.02007421851158142, 0.018253538757562637, 0.030845442786812782, -0.003916537389159203, -0.03722589462995529, -0.03793429955840111, -0.0366276390850544, 0.040177375078201294, 0.025641752406954765, 0.050752341747283936, 0.009178625419735909, 0.015644922852516174, -0.05116970092058182, 0.026423173025250435, -0.04617452621459961, -0.11185549944639206, 0.04559491202235222, 0.005629034712910652, 0.04989961162209511, 0.01582547090947628, 4.343787009520321e-33, -0.04824649542570114, 0.05515504628419876, -0.03658045455813408, 0.07717882096767426, -0.04245702549815178, -0.01731182262301445, -0.027591269463300705, 0.05471985787153244, 0.016260158270597458, 0.04789765179157257, 0.07744024693965912, -0.10876806825399399, 0.008191938512027264, -0.056485943496227264, 0.0007533782045356929, -0.019367031753063202, 0.06802394241094589, -0.018168283626437187, -0.07287456095218658, 0.0047272080555558205, -0.08330320566892624, -0.07571673393249512, -0.041829537600278854, 0.023495955392718315, -0.06861050426959991, 0.1004837229847908, -0.024190634489059448, 0.10863713920116425, -0.013029152527451515, 0.0865088626742363, 0.04757517948746681, -0.08774615824222565, -0.00682770274579525, 0.03047875314950943, 0.02873203717172146, 0.20470963418483734, 0.09231504052877426, 0.016746576875448227, -0.00757240504026413, 0.0051491581834852695, 0.042321279644966125, -0.025120457634329796, 0.02354445494711399, 0.13627475500106812, -0.04834604263305664, -0.012725894339382648, -0.004134716931730509, -0.007316081319004297, 0.08094491809606552, 0.030172914266586304, 0.025221839547157288, 0.03171887993812561, -0.025279255583882332, -0.05385499447584152, -0.017319928854703903, 0.02615794911980629, 0.03106723353266716, -0.018321603536605835, -0.03890589624643326, -0.024325508624315262, 0.03653610870242119, 0.018517831340432167, -0.004494012799113989, 0.060770463198423386, -0.008643320761620998, -0.034977179020643234, -0.04582889750599861, -0.018905285745859146, 0.005996147636324167, -0.01612183451652527, 0.06939784437417984, 0.003852604888379574, -0.0701839029788971, -0.08292588591575623, -0.026484305039048195, -0.08900762349367142, -0.022247757762670517, -0.013534908182919025, 0.0385512076318264, -0.039532650262117386, -0.05805931240320206, 0.019006885588169098, 0.0005042309057898819, -0.06959947198629379, -0.043609146028757095, 0.03509559854865074, 0.05783523619174957, -0.014005331322550774, -0.028270576149225235, 0.043782323598861694, -0.012300968170166016, 0.06069758161902428, 0.09069006890058517, -0.01602812483906746, 0.006933591794222593, -1.5034773426236825e-08, 0.05863410234451294, -0.024170150980353355, -0.026048174127936363, 0.06980574876070023, 0.06045081093907356, 0.006414224859327078, -0.05531464144587517, 0.012504199519753456, 0.04093146324157715, 0.028846364468336105, 0.01760147511959076, 0.005015002563595772, 0.023594560101628304, 0.0516316257417202, 0.0250548105686903, -0.05722063407301903, -0.12157945334911346, -0.00018272407760377973, -0.022113293409347534, -0.08405864983797073, -0.026487981900572777, 0.015580772422254086, -2.8786578695871867e-05, 0.023379845544695854, 0.0061817290261387825, 0.047046516090631485, -0.1017637699842453, 0.07688648253679276, 0.09898848831653595, -0.015477916225790977, 0.008368835784494877, 0.043241195380687714, -0.0442679338157177, 0.02451329305768013, -0.037223149091005325, -0.04206930100917816, -0.01472825463861227, 0.026079634204506874, -0.015963831916451454, 0.1360192447900772, -0.05150851234793663, 0.022148428484797478, 0.027097739279270172, -0.0014352109283208847, -0.06500371545553207, -0.020484279841184616, -0.04887637495994568, 0.0839962512254715, -0.0072325654327869415, -0.020550437271595, -0.06095936894416809, 0.06612943112850189, -0.00011498568346723914, 0.05219540372490883, 0.07801417261362076, 0.017105529084801674, 0.09189657866954803, 0.02236110344529152, -0.07054458558559418, 0.05508328601717949, 0.17235015332698822, -0.004390910267829895, 0.011533830314874649, -0.025606105104088783]\n",
      "Dimension of Embedding:  384\n"
     ]
    }
   ],
   "source": [
    "embedding = embedding_function.embed_documents(\"This is a test sentence.\")\n",
    "\n",
    "print(embedding[0])\n",
    "print(\"Dimension of Embedding: \", len(embedding[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chroma is an open source vector database capable of storing collections of documents along with their metadata, creating embeddings for documents and queries, and searching the collections filtering by document metadata or content. Additionally, Chroma supports multi-modal embedding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunks, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks in DB: 504\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunks in DB:\", db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Retrieval from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='examples, and it is the deﬁnition we use.\\nTwo contemporary works perform in-depth explorations of\\ntopics related to our work. Bojanowski & Joulin (2017)', metadata={'page': 7, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf'}),\n",
       " Document(page_content='layer. arXiv preprint arXiv:1701.06538 , 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-', metadata={'page': 21, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf'}),\n",
       " Document(page_content='1: Long Papers) , pages 434–443. ACL, August 2013.\\n12', metadata={'page': 21, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf'}),\n",
       " Document(page_content='networks. arXiv preprint arXiv:1605.08254 , 2016.\\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,\\nBruna, Joan, Erhan, Dumitru, Goodfellow, Ian J., and', metadata={'page': 9, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Give me the first chapter name.\"\n",
    "retriever = db.as_retriever()\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Connect to LLM (OpenAI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- openai_api_key=open_ai_token: Der API-Schlüssel für den Zugriff auf die OpenAI API.\n",
    "\n",
    "- model_name=\"gpt-3.5-turbo\": Der Name des zu verwendenden Modells.\n",
    "\n",
    "- temperature=0.0: Die \"Temperatur\" des Modells, die die Zufälligkeit der generierten Texte steuert. Ein Wert von 0.0 bedeutet, dass das - Modell immer die wahrscheinlichste Fortsetzung wählt.\n",
    "\n",
    "- max_tokens=100: Die maximale Anzahl von Tokens, die das Modell generieren soll.\n",
    "\n",
    "\n",
    "**- Was sind Tokens:**\n",
    "\n",
    "In der Textverarbeitung bezeichnet ein Token im Allgemeinen ein Stück eines größeren Textes, das als einzelne Einheit behandelt wird. Ein Token könnte ein Wort, ein Satzzeichen oder sogar ein einzelnes Zeichen sein, abhängig von der spezifischen Tokenisierungsmethode, die verwendet wird. In Ihrem Codebeispiel legt max_tokens=100 die maximale Anzahl von Tokens fest, die das Modell generieren soll. Das bedeutet, dass das Modell nicht mehr als 100 Tokens generieren wird, unabhängig davon, wie viele Wörter oder Zeichen das sind.\n",
    "\n",
    "**- Temperatures:**\n",
    "\n",
    "Bei der Textgenerierung steuert die \"Temperatur\" die Zufälligkeit der generierten Texte. Ein höherer Temperaturwert führt zu zufälligeren Texten, während ein niedrigerer Temperaturwert dazu führt, dass das Modell eher die wahrscheinlichsten Fortsetzungen wählt. In Ihrem Codebeispiel ist temperature=0.0 gesetzt, was bedeutet, dass das Modell immer die wahrscheinlichste Fortsetzung wählen wird. Dies kann dazu führen, dass die generierten Texte konsistenter, aber auch weniger vielfältig sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key= open_ai_token,\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0,\n",
    "    max_tokens = 100\n",
    ")\n",
    "\n",
    "# llm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-Instruct-v0.2', \n",
    "#                      huggingfacehub_api_token=api_token,  \n",
    "#                      model_kwargs={\"max_length\": 100})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Connect Vector DB to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",  \n",
    "    retriever = retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 RAG QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you summaries the Abstract in the paper 'Attention Is All You Need? '\",\n",
       " 'answer': \"The abstract in the paper 'Attention Is All You Need' discusses the use of self-attention in tasks such as reading comprehension and abstractive summarization. It also describes an attention function as mapping a query and a set of key-value pairs to an output.\\n\",\n",
       " 'sources': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can you summaries the Abstract in the paper 'Attention Is All You Need? '\"\n",
    "qa_with_sources(query)\n",
    "#llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Can you summaries the Abstract in the paper 'A Closer Look at Memorization in Deep Networks'\",\n",
       " 'answer': \"The abstract of the paper 'A Closer Look at Memorization in Deep Networks' discusses the analysis of memorization in deep networks and whether they use similar tactics on real datasets.\\n\",\n",
       " 'sources': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/merged.pdf'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can you summaries the Abstract in the paper 'A Closer Look at Memorization in Deep Networks'\"\n",
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
