{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Öffne die JSON-Datei und lade den Inhalt\n",
    "with open('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/api_token.json', 'r') as api_file:\n",
    "    api_token_file = json.load(api_file)\n",
    "\n",
    "# Extrahiere die Variable aus den Daten\n",
    "api_token = api_token_file['Hugging_face_token']\n",
    "open_ai_token = api_token_file['Open_api_token']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory,ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riccardo/anaconda3/envs/llm_train/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n",
      "/Users/riccardo/anaconda3/envs/llm_train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token=api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Where is Langchain?'\n",
    "output = llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is Langchain?\n",
      "\n",
      "Langchain is a decentralized language learning platform built on the Ethereum blockchain. It is not a physical location, but rather a digital ecosystem where users can access language learning resources, connect with language partners, and earn rewards for their learning progress.\n",
      "\n",
      "How does Langchain work?\n",
      "\n",
      "Langchain uses smart contracts to facilitate language learning and reward users for their progress. Users can access a variety of language learning resources, such as videos, articles, and interactive exercises,\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How does LangChain make LLM application development easier?',\n",
       " 'text': 'You are an artificial intelligence assistant, answer the question. How does LangChain make LLM application development easier?\\n\\nLangChain is a language model chain that utilizes multiple language models to generate more accurate and contextually relevant responses. It makes LLM (Large Language Model) application development easier in several ways:\\n\\n1. Improved Accuracy: By chaining multiple language models, LangChain can generate more accurate responses by leveraging the strengths of each model. This reduces the need for extensive fine-tuning and manual intervention in the development process.\\n2. Contextual Understanding:'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riccardo/anaconda3/envs/llm_train/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/riccardo/anaconda3/envs/llm_train/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"A list comprehension is a concise way to create lists in Python. It allows you to generate a new list by applying an expression to each item in an existing iterable (such as a list, tuple, or range) and optionally filtering the items based on a condition.\\n\\nHere's a simple example of a list comprehension that generates a list of squared numbers from 0 to 9:\\n\\n```python\\nsquared_numbers = [x**2 for x in range(10)]\\nprint(squared_numbers)\\n```\\n\\nThis will output:\\n\\n```\\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\\n```\\n\\nList comprehensions are a powerful and concise way to work with lists in Python and are often preferred over traditional for loops for their readability and simplicity.\"\n",
      "content='List comprehension is a concise way to create lists in Python by applying an expression to each item in an iterable.'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set your API Key from OpenAI\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Create the conversation history and add the first AI message\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hello! Ask me anything about Python programming!\")\n",
    "\n",
    "# Add the user message to the history and call the model\n",
    "history.add_user_message(\"What is a list comprehension?\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)\n",
    "\n",
    "# Add another user message and call the model\n",
    "history.add_user_message(\"Describe the same in fewer words\")\n",
    "\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Explain what is seaborn in python.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Explain what is seaborn in python.\n",
      "AI:  Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. It is built on top of matplotlib and integrates closely with pandas data structures. Seaborn offers a variety of plot types, including scatter plots, line plots, bar charts, histograms, and heatmaps. It also has built-in support for visualizing categorical data and time series data. Seaborn is widely used in data science and machine learning projects for exploratory data analysis and communicating insights.\n",
      "Human: Tell me about Germany something\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Explain what is seaborn in python.\n",
      "AI:  Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. It is built on top of matplotlib and integrates closely with pandas data structures. Seaborn offers a variety of plot types, including scatter plots, line plots, bar charts, histograms, and heatmaps. It also has built-in support for visualizing categorical data and time series data. Seaborn is widely used in data science and machine learning projects for exploratory data analysis and communicating insights.\n",
      "Human: Tell me about Germany something\n",
      "AI:  Germany is a country located in central-western Europe. It has a population of over 83 million people and is the most populous country in the European Union. The official language is German, and the currency is the Euro. Germany has a rich history and is known for its contributions to science, philosophy, and the arts. It is also a major economic power, with a strong manufacturing sector and a highly skilled workforce. Some famous landmarks in Germany include the Brandenburg Gate, the Cologne Cathedral, and Neuschwanstein Castle. The country is also known for its delicious cuisine, including dishes such as bratwurst, sauerkraut, and schnitzel.\n",
      "Human: Tell me about Xbox something.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Explain what is seaborn in python.\n",
      "AI:  Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. It is built on top of matplotlib and integrates closely with pandas data structures. Seaborn offers a variety of plot types, including scatter plots, line plots, bar charts, histograms, and heatmaps. It also has built-in support for visualizing categorical data and time series data. Seaborn is widely used in data science and machine learning projects for exploratory data analysis and communicating insights.\n",
      "Human: Tell me about Germany something\n",
      "AI:  Germany is a country located in central-western Europe. It has a population of over 83 million people and is the most populous country in the European Union. The official language is German, and the currency is the Euro. Germany has a rich history and is known for its contributions to science, philosophy, and the arts. It is also a major economic power, with a strong manufacturing sector and a highly skilled workforce. Some famous landmarks in Germany include the Brandenburg Gate, the Cologne Cathedral, and Neuschwanstein Castle. The country is also known for its delicious cuisine, including dishes such as bratwurst, sauerkraut, and schnitzel.\n",
      "Human: Tell me about Xbox something.\n",
      "AI:  Xbox is a video game console brand created and owned by Microsoft. The first Xbox console was released in 2001, and it has since gone through several iterations, including the Xbox 360 and the current Xbox One. Xbox consoles are known for their high-quality graphics and immersive gaming experiences. They also offer online multiplayer capabilities and a wide range of entertainment options, such as streaming services and music apps. Xbox is a popular choice among gamers worldwide and has a large library of exclusive games, including the popular Halo and Gears of War franchises.\n",
      "Human: Tell me about Playstation something.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Explain what is seaborn in python.\n",
      "AI:  Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. It is built on top of matplotlib and integrates closely with pandas data structures. Seaborn offers a variety of plot types, including scatter plots, line plots, bar charts, histograms, and heatmaps. It also has built-in support for visualizing categorical data and time series data. Seaborn is widely used in data science and machine learning projects for exploratory data analysis and communicating insights.\n",
      "Human: Tell me about Germany something\n",
      "AI:  Germany is a country located in central-western Europe. It has a population of over 83 million people and is the most populous country in the European Union. The official language is German, and the currency is the Euro. Germany has a rich history and is known for its contributions to science, philosophy, and the arts. It is also a major economic power, with a strong manufacturing sector and a highly skilled workforce. Some famous landmarks in Germany include the Brandenburg Gate, the Cologne Cathedral, and Neuschwanstein Castle. The country is also known for its delicious cuisine, including dishes such as bratwurst, sauerkraut, and schnitzel.\n",
      "Human: Tell me about Xbox something.\n",
      "AI:  Xbox is a video game console brand created and owned by Microsoft. The first Xbox console was released in 2001, and it has since gone through several iterations, including the Xbox 360 and the current Xbox One. Xbox consoles are known for their high-quality graphics and immersive gaming experiences. They also offer online multiplayer capabilities and a wide range of entertainment options, such as streaming services and music apps. Xbox is a popular choice among gamers worldwide and has a large library of exclusive games, including the popular Halo and Gears of War franchises.\n",
      "Human: Tell me about Playstation something.\n",
      "AI:  Playstation is a video game console brand created and owned by Sony Interactive Entertainment. The first Playstation console was released in 1994, and it has since gone through several iterations, including the Playstation 2, Playstation 3, and the current Playstation 4. Playstation consoles are known for their high-quality graphics and exclusive game titles, such as the popular God of War and Uncharted series. They also offer online multiplayer capabilities and a variety of entertainment options, including streaming services and virtual reality experiences. Playstation is a popular choice among gamers worldwide and has a large and dedicated fan base.\n",
      "Human: What was my first question?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Your first question was \"Explain what is seaborn in python.\"'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set your API Key from OpenAI\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Define a buffer memory\n",
    "memory = ConversationBufferMemory(size=2)\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "buffer_chain.predict(input=\"Explain what is seaborn in python.\")\n",
    "buffer_chain.predict(input=\"Tell me about Germany something\")\n",
    "buffer_chain.predict(input=\"Tell me about Xbox something.\")\n",
    "buffer_chain.predict(input=\"Tell me about Playstation something.\")\n",
    "buffer_chain.predict(input=\"What was my first question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Describe the relationship of the human mind with the keyboard when taking a great online class.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human asks the AI to describe the relationship between the human mind and the keyboard in taking a great online class. The AI explains that it is one of collaboration and communication, with the keyboard acting as a tool for input and interaction. The keyboard serves as a bridge between the human mind and the online class, allowing for a smooth flow of information and learning. It also allows for the expression of thoughts and ideas in written form, enhancing the learning experience.\n",
      "Human: Use an analogy to describe it.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sure, I would say the relationship between the human mind and the keyboard in taking an online class is like a conductor and their baton in an orchestra. The conductor uses the baton to guide and direct the musicians, just as the keyboard guides and directs the flow of information and communication between the human mind and the online class. Both are essential tools for creating a harmonious and successful experience.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set your API Key from OpenAI\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Define a summary memory that uses an OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=open_ai_token))\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "summary_chain.predict(input=\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
    "summary_chain.predict(input=\"Use an analogy to describe it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Retrieval Augmented Generation (RAG)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='  Who  21\\nhelp you ensure they hear your message. Second, What do you want \\nyour audience to know or do?  You should be clear how you want your \\naudience to act and take into account how you will communicate to \\nthem and the overall tone that you want to set for your communication.\\nIt’s only after you can concisely answer these first two questions that \\nyou’re ready to move forward with the third: How can you use data \\nto help make your point?\\nLet’s look at the context of who, what, and how in a little more detail.\\nWho\\nYour audience\\nThe more specific you can be about who your audience is, the better \\nposition you will be in for successful communication. Avoid general \\naudiences, such as “internal and external stakeholders” or “anyone \\nwho might be interested”—by trying to communicate to too many \\ndifferent people with disparate needs at once, you put yourself in a \\nposition where you can’t communicate to any one of them as effec-\\ntively as you could if you narrowed your target audience. Sometimes \\nthis means creating different communications for different audi -\\nences. Identifying the decision maker is one way of narrowing your \\naudience. The more you know about your audience, the better posi-\\ntioned you’ll be to understand how to resonate with them and form \\na communication that will meet their needs and yours.\\nYou\\nIt’s also helpful to think about the relationship that you have with \\nyour audience and how you expect that they will perceive you. Will \\nyou be encountering each other for the first time through this com-\\nmunication, or do you have an established relationship? Do they \\nalready trust you as an expert, or do you need to work to establish \\ncredibility? These are important considerations when it comes to ' metadata={'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf', 'page': 36}\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf\")\n",
    "data = loader.load()\n",
    "print(data[36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third party document loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='I no longer maintain my Emacs projects on Sourcehut (protesilaos.com)' metadata={'source': 'https://news.ycombinator.com/', 'title': 'I no longer maintain my Emacs projects on Sourcehut (protesilaos.com)', 'link': 'https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/', 'ranking': '1.'}\n",
      "{'source': 'https://news.ycombinator.com/', 'title': 'I no longer maintain my Emacs projects on Sourcehut (protesilaos.com)', 'link': 'https://protesilaos.com/codelog/2024-01-27-sourcehut-no-more/', 'ranking': '1.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import HNLoader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/\")\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting by Charakter**\n",
    "\n",
    "Dieser Code erstellt eine Instanz der CharacterTextSplitter-Klasse und verwendet sie, um einen gegebenen Text in Chunks aufzuteilen.\n",
    "\n",
    "Zuerst wird eine Zeichenkette quote definiert, die den Text enthält, der aufgeteilt werden soll. Dann werden die Variablen chunk_size und chunk_overlap festgelegt, um die Größe der Chunks und die Überlappung zwischen den Chunks anzugeben.\n",
    "\n",
    "Anschließend wird eine Instanz der CharacterTextSplitter-Klasse erstellt und den Variablen chunk_size und chunk_overlap übergeben. Diese Klasse implementiert die Logik zum Aufteilen des Textes in Chunks.\n",
    "\n",
    "Schließlich wird die Methode split_text() der splitter-Instanz aufgerufen, um den Text in Chunks aufzuteilen. Das Ergebnis wird in der Variable docs gespeichert. Schließlich werden die Chunks mit print(docs) ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.']\n"
     ]
    }
   ],
   "source": [
    "quote = 'One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote) \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Words are flowing out', 'out like endless rain', 'rain into a paper cup,', 'they slither while they', 'they pass,', 'they slip away across', 'across the universe.']\n"
     ]
    }
   ],
   "source": [
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_overlap = 10,\n",
    "    chunk_size=24\n",
    "\n",
    ")\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m loader \u001b[38;5;241m=\u001b[39m UnstructuredHTMLLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define variables\u001b[39;00m\n\u001b[1;32m      5\u001b[0m chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/langchain_core/document_loaders/base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/langchain_community/document_loaders/unstructured.py:87\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_elements()\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/langchain_community/document_loaders/html.py:33\u001b[0m, in \u001b[0;36mUnstructuredHTMLLoader._get_elements\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_html\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_html(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munstructured_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/documents/elements.py:526\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 526\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    527\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    528\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/file_utils/filetype.py:622\u001b[0m, in \u001b[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 622\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    623\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    624\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/file_utils/filetype.py:582\u001b[0m, in \u001b[0;36madd_metadata.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[0;32m--> 582\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    583\u001b[0m     sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(func)\n\u001b[1;32m    584\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args)), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/chunking/dispatch.py:83\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_args\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[1;32m     86\u001b[0m call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults()\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/partition/html.py:101\u001b[0m, in \u001b[0;36mpartition_html\u001b[0;34m(filename, file, text, url, encoding, include_page_breaks, include_metadata, headers, ssl_verify, parser, source_format, html_assemble_articles, metadata_filename, metadata_last_modified, skip_headers_and_footers, chunking_strategy, languages, detect_language_per_element, detection_origin, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     last_modification_date \u001b[38;5;241m=\u001b[39m get_last_modified_date(filename)\n\u001b[0;32m--> 101\u001b[0m     document \u001b[38;5;241m=\u001b[39m HTMLDocument\u001b[38;5;241m.\u001b[39mfrom_file(\n\u001b[1;32m    102\u001b[0m         filename,\n\u001b[1;32m    103\u001b[0m         parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[1;32m    104\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    105\u001b[0m         assemble_articles\u001b[38;5;241m=\u001b[39mhtml_assemble_articles,\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     last_modification_date \u001b[38;5;241m=\u001b[39m get_last_modified_date_from_file(file)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/documents/xml.py:119\u001b[0m, in \u001b[0;36mXMLDocument.from_file\u001b[0;34m(cls, filename, parser, stylesheet, encoding, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_file\u001b[39m(\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    118\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 119\u001b[0m     _, content \u001b[38;5;241m=\u001b[39m read_txt_file(filename\u001b[38;5;241m=\u001b[39mfilename, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_string(content, parser\u001b[38;5;241m=\u001b[39mparser, stylesheet\u001b[38;5;241m=\u001b[39mstylesheet, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/file_utils/encoding.py:123\u001b[0m, in \u001b[0;36mread_txt_file\u001b[0;34m(filename, file, encoding)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m         formatted_encoding, file_text \u001b[38;5;241m=\u001b[39m detect_file_encoding(filename)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m file:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm_train/lib/python3.11/site-packages/unstructured/file_utils/encoding.py:66\u001b[0m, in \u001b[0;36mdetect_file_encoding\u001b[0;34m(filename, file)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_file_encoding\u001b[39m(\n\u001b[1;32m     62\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m     file: Optional[Union[\u001b[38;5;28mbytes\u001b[39m, IO[\u001b[38;5;28mbytes\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m---> 66\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     67\u001b[0m             byte_data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m file:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/'"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredHTMLLoader(\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Define variables\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Split the HTML\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.'])\n",
    "\n",
    "docs = splitter.split_documents(data)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = open_ai_token\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf')\n",
    "data = loader.load()\n",
    "print(data[89]) ## seite 74\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Split the quote using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "docs = splitter.split_documents(data) \n",
    "docs\n",
    "# Define an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=open_ai_token)\n",
    "\n",
    "# Create the Chroma vector DB using the OpenAI embedding function; persist the database\n",
    "vectordb = Chroma(\n",
    "    persist_directory='embedding/chroma/',\n",
    "    embedding_function=embedding_model)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key),         chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the chain on the query provided\n",
    "query = \"what is the third chapter of storytelling-with-data-cole-nussbaumer-knaflic called?\"\n",
    "print(qa.invoke(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf')\n",
    "data = loader.load()\n",
    "data\n",
    "# print(data[1])\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200,\n",
    "#     chunk_overlap=50,\n",
    "#     separators=['.'])\n",
    "# docs = splitter.split_documents(data) \n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "# docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# # Define the function for the question to be answered with\n",
    "# qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "#     OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, \n",
    "#            openai_api_key=openai_api_key), chain_type=\"stuff\", \n",
    "#            retriever=docstorage.as_retriever()\n",
    "# )\n",
    "\n",
    "# # Run the query on the documents\n",
    "# results = qa({\"question\": \"Which models does he explain in the first pages?\"}, return_only_outputs=True)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import HuggingFaceHub\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Setze die Repository-ID für Mistral-7B-Instruct\n",
    "repo_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "# Erstelle eine Instanz von HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=api_token)\n",
    "\n",
    "# Lade nur die Seite 5 des PDF-Dokuments\n",
    "loader = PyPDFLoader(\"/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf\")\n",
    "page_number = 5\n",
    "data = loader.load_page(page_number)\n",
    "\n",
    "# Erstelle eine Instanz von HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='bert-base-uncased')  # Beispiel-Modellname, anpassen nach Bedarf\n",
    "\n",
    "# Generiere die Einbettungen für die geladene Seite\n",
    "embeddings = embedding_model.embed_text(data)\n",
    "\n",
    "\n",
    "# # Definiere die Größe der Abschnitte für den Splitter\n",
    "# chunk_size = 200\n",
    "# chunk_overlap = 50\n",
    "\n",
    "# # Teile den Text mit RecursiveCharacterTextSplitter\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=chunk_size,\n",
    "#     chunk_overlap=chunk_overlap)\n",
    "# docs = splitter.split_documents(data) \n",
    "\n",
    "# # Erstelle eine Instanz von HuggingFaceEmbeddings\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name='bert-base-uncased')  # Beispiel-Modellname, anpassen nach Bedarf\n",
    "\n",
    "# # Verwende die Embedding-Methode von HuggingFaceEmbeddings\n",
    "# embeddings = embedding_model.embed_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter\n",
    "\n",
    "- Unterteil ein Text in 300 Chunks. Ein Chunk ist ein Charakter eines Wortes das heißt es werden 300 Zeichen aus der Textpassage genommen.\n",
    "- Weiterhin setzten wir die Größe des Overlaps auf 100 Zeichen. Das heißt das von einem Chunk zum anderen Chunk 100 Zeichen überlappen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50, \n",
    "    length_function = len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **filepath = '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf':** Hier wird der Pfad zur PDF-Datei festgelegt, die gelesen werden soll.\n",
    "\n",
    "- **loader = PyPDFLoader(filepath):** Hier wird eine Instanz der PyPDFLoader-Klasse erstellt, die zum Laden von PDF-Dateien verwendet wird. Der Pfad zur PDF-Datei wird als Argument an den Konstruktor der Klasse übergeben.\n",
    "\n",
    "- **chunks = loader.load_and_split(text_splitter=text_splitter):** Die Methode load_and_split der PyPDFLoader-Instanz wird aufgerufen, um die PDF-Datei zu laden und ihren Inhalt in Chunks aufzuteilen. Der text_splitter, der zuvor als Instanz der RecursiveCharacterTextSplitter-Klasse erstellt wurde, wird als Argument an die Methode übergeben und bestimmt, wie der Text in Chunks aufgeteilt wird. Die aufgeteilten Chunks werden in der Variable chunks gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf'\n",
    "loader = PyPDFLoader(filepath)\n",
    "chunks = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Display Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks[79:90]:\n",
    "    print(\"Page content: \\n\", chunk.page_content),\n",
    "    print(\"Page_metadata: \\n\", chunk.metadata),\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB \n",
    "## 1. load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Use an open source embedding model to embed the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In dieser Zeile wird eine Instanz der Klasse SentenceTransformerEmbeddings erstellt. Dies ist ein Modell, das Sätze in Vektoren umwandelt, die ihre semantische Bedeutung repräsentieren. Diese Vektoren werden oft als \"Einbettungen\" oder \"Embeddings\" bezeichnet.\n",
    "\n",
    "- Die SentenceTransformerEmbeddings-Klasse verwendet die SentenceTransformer-Bibliothek, um Einbettungen für Texte zu generieren. Die Klasse verwendet ein vortrainiertes Modell, um die Einbettungen zu generieren. Die Einbettungen können dann in einer Chroma-Datenbank gespeichert und für die spätere Verwendung abgerufen werden.\n",
    "\n",
    "- Der Parameter model_name=\"all-MiniLM-L6-v2\" gibt an, welches vortrainierte Modell verwendet werden soll. In diesem Fall wird das Modell \"all-MiniLM-L6-v2\" verwendet, das ein kleines, aber leistungsfähiges Modell für die Erzeugung von Satzeinbettungen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_function.embed_documents(\"This is a test sentence.\")\n",
    "\n",
    "print(embedding[0])\n",
    "print(\"Dimension of Embedding: \", len(embedding[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chroma is an open source vector database capable of storing collections of documents along with their metadata, creating embeddings for documents and queries, and searching the collections filtering by document metadata or content. Additionally, Chroma supports multi-modal embedding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunks, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chunks in DB:\", db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Retrieval from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Give me the first chapter name.\"\n",
    "retriever = db.as_retriever()\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Connect to LLM (OpenAI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- openai_api_key=open_ai_token: Der API-Schlüssel für den Zugriff auf die OpenAI API.\n",
    "\n",
    "- model_name=\"gpt-3.5-turbo\": Der Name des zu verwendenden Modells.\n",
    "\n",
    "- temperature=0.0: Die \"Temperatur\" des Modells, die die Zufälligkeit der generierten Texte steuert. Ein Wert von 0.0 bedeutet, dass das - Modell immer die wahrscheinlichste Fortsetzung wählt.\n",
    "\n",
    "- max_tokens=100: Die maximale Anzahl von Tokens, die das Modell generieren soll.\n",
    "\n",
    "\n",
    "**- Was sind Tokens:**\n",
    "\n",
    "In der Textverarbeitung bezeichnet ein Token im Allgemeinen ein Stück eines größeren Textes, das als einzelne Einheit behandelt wird. Ein Token könnte ein Wort, ein Satzzeichen oder sogar ein einzelnes Zeichen sein, abhängig von der spezifischen Tokenisierungsmethode, die verwendet wird. In Ihrem Codebeispiel legt max_tokens=100 die maximale Anzahl von Tokens fest, die das Modell generieren soll. Das bedeutet, dass das Modell nicht mehr als 100 Tokens generieren wird, unabhängig davon, wie viele Wörter oder Zeichen das sind.\n",
    "\n",
    "**- Temperatures:**\n",
    "\n",
    "Bei der Textgenerierung steuert die \"Temperatur\" die Zufälligkeit der generierten Texte. Ein höherer Temperaturwert führt zu zufälligeren Texten, während ein niedrigerer Temperaturwert dazu führt, dass das Modell eher die wahrscheinlichsten Fortsetzungen wählt. In Ihrem Codebeispiel ist temperature=0.0 gesetzt, was bedeutet, dass das Modell immer die wahrscheinlichste Fortsetzung wählen wird. Dies kann dazu führen, dass die generierten Texte konsistenter, aber auch weniger vielfältig sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(\n",
    "#     openai_api_key= open_ai_token,\n",
    "#     model_name = \"gpt-3.5-turbo\",\n",
    "#     temperature = 0.0,\n",
    "#     max_tokens = 100\n",
    "# )\n",
    "\n",
    "llm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-Instruct-v0.2', \n",
    "                     huggingfacehub_api_token=api_token,  \n",
    "                     model_kwargs={\"max_length\": 100})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Connect Vector DB to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",  \n",
    "    retriever = retriever\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 RAG QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the title of the book.\"\n",
    "qa_with_sources(query)\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
