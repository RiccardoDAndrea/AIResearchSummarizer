{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Öffne die JSON-Datei und lade den Inhalt\n",
    "with open('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/api_token.json', 'r') as api_file:\n",
    "    api_token_file = json.load(api_file)\n",
    "\n",
    "# Extrahiere die Variable aus den Daten\n",
    "api_token = api_token_file['Hugging_face_token']\n",
    "open_ai_token = api_token_file['Open_api_token']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory,ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id='mistralai/Mistral-7B-Instruct-v0.2', huggingfacehub_api_token=api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Where is Langchain?'\n",
    "output = llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "llm_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your API Key from OpenAI\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Create the conversation history and add the first AI message\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hello! Ask me anything about Python programming!\")\n",
    "\n",
    "# Add the user message to the history and call the model\n",
    "history.add_user_message(\"What is a list comprehension?\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)\n",
    "\n",
    "# Add another user message and call the model\n",
    "history.add_user_message(\"Describe the same in fewer words\")\n",
    "\n",
    "\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Define a buffer memory\n",
    "memory = ConversationBufferMemory(size=2)\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "buffer_chain.predict(input=\"Explain what is seaborn in python.\")\n",
    "buffer_chain.predict(input=\"Tell me about Germany something\")\n",
    "buffer_chain.predict(input=\"Tell me about Xbox something.\")\n",
    "buffer_chain.predict(input=\"Tell me about Playstation something.\")\n",
    "buffer_chain.predict(input=\"What was my first question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=open_ai_token)\n",
    "\n",
    "# Define a summary memory that uses an OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=open_ai_token))\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "summary_chain.predict(input=\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
    "summary_chain.predict(input=\"Use an analogy to describe it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Retrieval Augmented Generation (RAG)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf\")\n",
    "data = loader.load()\n",
    "print(data[36])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third party document loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import HNLoader\n",
    "loader = HNLoader(\"https://news.ycombinator.com/\")\n",
    "data = loader.load()\n",
    "print(data[0])\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting by Charakter**\n",
    "\n",
    "Dieser Code erstellt eine Instanz der CharacterTextSplitter-Klasse und verwendet sie, um einen gegebenen Text in Chunks aufzuteilen.\n",
    "\n",
    "Zuerst wird eine Zeichenkette quote definiert, die den Text enthält, der aufgeteilt werden soll. Dann werden die Variablen chunk_size und chunk_overlap festgelegt, um die Größe der Chunks und die Überlappung zwischen den Chunks anzugeben.\n",
    "\n",
    "Anschließend wird eine Instanz der CharacterTextSplitter-Klasse erstellt und den Variablen chunk_size und chunk_overlap übergeben. Diese Klasse implementiert die Logik zum Aufteilen des Textes in Chunks.\n",
    "\n",
    "Schließlich wird die Methode split_text() der splitter-Instanz aufgerufen, um den Text in Chunks aufzuteilen. Das Ergebnis wird in der Variable docs gespeichert. Schließlich werden die Chunks mit print(docs) ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote) \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_overlap = 10,\n",
    "    chunk_size=24\n",
    "\n",
    ")\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader(\"https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Define variables\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Split the HTML\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.'])\n",
    "\n",
    "docs = splitter.split_documents(data)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "openai_api_key = open_ai_token\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf')\n",
    "data = loader.load()\n",
    "print(data[89]) ## seite 74\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Split the quote using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "docs = splitter.split_documents(data) \n",
    "docs\n",
    "# Define an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=open_ai_token)\n",
    "\n",
    "# Create the Chroma vector DB using the OpenAI embedding function; persist the database\n",
    "vectordb = Chroma(\n",
    "    persist_directory='embedding/chroma/',\n",
    "    embedding_function=embedding_model)\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=['.'])\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents and store them in a Chroma DB\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# Define the Retrieval QA Chain to integrate the database and LLM\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key),         chain_type=\"stuff\", retriever=docstorage.as_retriever())\n",
    "\n",
    "# Run the chain on the query provided\n",
    "query = \"what is the third chapter of storytelling-with-data-cole-nussbaumer-knaflic called?\"\n",
    "print(qa.invoke(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key from OpenAI\n",
    "\n",
    "loader = PyPDFLoader('/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf')\n",
    "data = loader.load()\n",
    "data\n",
    "# print(data[1])\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=200,\n",
    "#     chunk_overlap=50,\n",
    "#     separators=['.'])\n",
    "# docs = splitter.split_documents(data) \n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "# docstorage = Chroma.from_documents(docs, embedding_model)\n",
    "\n",
    "# # Define the function for the question to be answered with\n",
    "# qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "#     OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, \n",
    "#            openai_api_key=openai_api_key), chain_type=\"stuff\", \n",
    "#            retriever=docstorage.as_retriever()\n",
    "# )\n",
    "\n",
    "# # Run the query on the documents\n",
    "# results = qa({\"question\": \"Which models does he explain in the first pages?\"}, return_only_outputs=True)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import HuggingFaceHub\n",
    "from langchain.load.py_loader import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Setze die Repository-ID für Mistral-7B-Instruct\n",
    "repo_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "# Erstelle eine Instanz von HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id=repo_id, huggingfacehub_api_token=api_token)\n",
    "\n",
    "# Lade nur die Seite 5 des PDF-Dokuments\n",
    "loader = PyPDFLoader(\"/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/storytelling-with-data-cole-nussbaumer-knaflic.pdf\")\n",
    "page_number = 5\n",
    "data = loader.load_page(page_number)\n",
    "\n",
    "# Erstelle eine Instanz von HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='bert-base-uncased')  # Beispiel-Modellname, anpassen nach Bedarf\n",
    "\n",
    "# Generiere die Einbettungen für die geladene Seite\n",
    "embeddings = embedding_model.embed_text(data)\n",
    "\n",
    "\n",
    "# # Definiere die Größe der Abschnitte für den Splitter\n",
    "# chunk_size = 200\n",
    "# chunk_overlap = 50\n",
    "\n",
    "# # Teile den Text mit RecursiveCharacterTextSplitter\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=chunk_size,\n",
    "#     chunk_overlap=chunk_overlap)\n",
    "# docs = splitter.split_documents(data) \n",
    "\n",
    "# # Erstelle eine Instanz von HuggingFaceEmbeddings\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name='bert-base-uncased')  # Beispiel-Modellname, anpassen nach Bedarf\n",
    "\n",
    "# # Verwende die Embedding-Methode von HuggingFaceEmbeddings\n",
    "# embeddings = embedding_model.embed_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf'\n",
    "loader = PyPDFLoader(filepath)\n",
    "chunks = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Display Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page content: \n",
      " war fasziniert, dass es möglich ist mit den Daten Prädiktionen abzugeben. In den Vorlesungen ist mir bewusst geworden, dass ich mich in diesem Bereich weiter entwickeln möchte und auch mein Praktikum als Data Scientisten absolvieren werde. Aus diesen Grund habe ich mich bei der Marketing-Agentur\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf', 'page': 3}\n",
      "----------------------------\n",
      "Page content: \n",
      " Grund habe ich mich bei der Marketing-Agentur mso digital GmbH & Co. KG (mso digital), für den Praktikumszeitraum vom 01.03.2023 bis zum 04.07.2023, in der Abteilung Data & Process Analytics hier in Osnabrück beworben. 2. Vorstellung der mso digital GmbH & Co. KG 2.1 Das Unternehmen Die mso digital\n",
      "Page_metadata: \n",
      " {'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf', 'page': 3}\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks[12:14]:\n",
    "    print(\"Page content: \\n\", chunk.page_content),\n",
    "    print(\"Page_metadata: \\n\", chunk.metadata),\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector DB \n",
    "## 1. load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Use an open source embedding model to embed the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riccardo/anaconda3/envs/llm_train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05895749852061272, 0.05387335643172264, 0.008316715247929096, 0.0206805057823658, 0.0022009313106536865, -0.034434445202350616, 0.08473188430070877, 0.1077682226896286, 0.058957234025001526, -0.016638653352856636, 0.06211813539266586, -0.0740492045879364, -0.0303032286465168, 0.03807591274380684, -0.004251338075846434, -0.007986415177583694, 0.0031452886760234833, -0.07220274955034256, -0.01501194667071104, -0.07136189937591553, -0.08817413449287415, 0.0452018640935421, 0.009071135893464088, 0.041956428438425064, -0.0012928870273754, -0.014840386807918549, -0.03942627087235451, 0.007077477872371674, -0.036413680762052536, -0.042899180203676224, -0.04982227832078934, 0.009418361820280552, -0.022699205204844475, 0.010238043032586575, -0.00445396825671196, -0.07801611721515656, -0.070695661008358, -0.005806431174278259, 0.03875807300209999, 0.01204211637377739, -0.05325385555624962, -0.0963793396949768, -0.028097203001379967, -0.009883742779493332, 0.06731418520212173, 0.004278208594769239, 0.02124866470694542, -0.022914476692676544, -0.06625277549028397, -0.02348857931792736, 4.158759111305699e-05, 0.00964048970490694, -0.03294288367033005, 0.014496402814984322, 0.018593674525618553, 0.047485243529081345, -0.012361058965325356, 0.03878827765583992, 0.02369307167828083, -0.06135307252407074, -0.002361031249165535, -0.03253591060638428, -0.11364953964948654, 0.06236247345805168, 0.059831637889146805, 0.08645803481340408, -0.022679559886455536, 0.03328515589237213, 0.016969120129942894, -0.04824632778763771, -0.010130156762897968, -0.0021239817142486572, -0.06585216522216797, -0.04025018960237503, -0.021393608301877975, -0.02657456323504448, 0.046205248683691025, -0.044886842370033264, 0.034001607447862625, 0.05156944692134857, -0.006135378964245319, 0.001145144458860159, -0.09618978202342987, 0.029961099848151207, -0.007510695140808821, 0.015603425912559032, -0.042808204889297485, 0.043429456651210785, -0.019901875406503677, 0.07665636390447617, -0.01384327095001936, 0.034392017871141434, 0.07292252033948898, 0.0133975213393569, -0.07558221369981766, -0.03218085318803787, -0.05393291264772415, -0.016610195860266685, -0.10869736224412918, 0.23937205970287323, -0.020459236577153206, 0.014236437156796455, 0.02164141833782196, 0.09602873772382736, -0.010307742282748222, 0.01200162898749113, -0.018780171871185303, -0.04145337641239166, 0.048597563058137894, -0.005922127515077591, -0.06287381052970886, -0.03733189404010773, 0.013004820793867111, -0.009868098422884941, 0.048202723264694214, -0.02079993113875389, -0.08003193140029907, 0.005167827010154724, 0.00725804315879941, -0.014367008581757545, 0.06591780483722687, 0.04329393804073334, -0.06329438835382462, 0.014587262645363808, -0.003063279204070568, -0.03614052012562752, 0.03620026260614395, -4.612938114318177e-33, 0.0409914031624794, 0.004939346108585596, -0.006083912216126919, 0.043096765875816345, 0.0167239923030138, 0.09288540482521057, -0.03938951715826988, 0.004293610341846943, -0.05639263615012169, 0.0672842413187027, -0.16096237301826477, -0.0779045820236206, -0.03543432056903839, 0.0018369597382843494, 0.06707488000392914, 0.028946315869688988, 0.03300480917096138, 0.029375018551945686, -0.0051882704719901085, -0.027864258736371994, -0.01788497157394886, 0.1109727993607521, -0.04470197111368179, 0.04207243770360947, 0.011976080946624279, -0.08128233253955841, 0.008002789691090584, -0.07154875248670578, -0.06217927113175392, 0.033613819628953934, -0.01154992263764143, 0.027502810582518578, -0.07996906340122223, 0.002480847528204322, 0.02366090752184391, -0.09160154312849045, 0.06299559026956558, -0.004837492015212774, -0.0030987428035587072, -0.024675210937857628, 0.08662998676300049, 0.024653417989611626, -9.652456355979666e-05, -0.03318057581782341, -0.009754078462719917, 0.08528172969818115, 0.09049248695373535, -0.005956639070063829, 0.011446849443018436, 0.042310360819101334, 0.0035262685269117355, -0.01033497042953968, -0.07504887878894806, -0.012202832847833633, 0.03234664723277092, 0.025926638394594193, 0.019049691036343575, -0.012024185620248318, 0.05401187390089035, 0.01013104896992445, 0.04285314679145813, -0.040263108909130096, 0.013790000230073929, -0.06209595873951912, -0.009781830944120884, -0.033753663301467896, -0.02063378319144249, -0.03565194085240364, 0.05844387784600258, -0.0428612157702446, -0.05484124273061752, 0.0035845881793648005, 0.0965285524725914, -0.039651673287153244, -0.008388887159526348, -0.02007421851158142, 0.018253538757562637, 0.030845442786812782, -0.003916537389159203, -0.03722589462995529, -0.03793429955840111, -0.0366276390850544, 0.040177375078201294, 0.025641752406954765, 0.050752341747283936, 0.009178625419735909, 0.015644922852516174, -0.05116970092058182, 0.026423173025250435, -0.04617452621459961, -0.11185549944639206, 0.04559491202235222, 0.005629034712910652, 0.04989961162209511, 0.01582547090947628, 4.343787009520321e-33, -0.04824649542570114, 0.05515504628419876, -0.03658045455813408, 0.07717882096767426, -0.04245702549815178, -0.01731182262301445, -0.027591269463300705, 0.05471985787153244, 0.016260158270597458, 0.04789765179157257, 0.07744024693965912, -0.10876806825399399, 0.008191938512027264, -0.056485943496227264, 0.0007533782045356929, -0.019367031753063202, 0.06802394241094589, -0.018168283626437187, -0.07287456095218658, 0.0047272080555558205, -0.08330320566892624, -0.07571673393249512, -0.041829537600278854, 0.023495955392718315, -0.06861050426959991, 0.1004837229847908, -0.024190634489059448, 0.10863713920116425, -0.013029152527451515, 0.0865088626742363, 0.04757517948746681, -0.08774615824222565, -0.00682770274579525, 0.03047875314950943, 0.02873203717172146, 0.20470963418483734, 0.09231504052877426, 0.016746576875448227, -0.00757240504026413, 0.0051491581834852695, 0.042321279644966125, -0.025120457634329796, 0.02354445494711399, 0.13627475500106812, -0.04834604263305664, -0.012725894339382648, -0.004134716931730509, -0.007316081319004297, 0.08094491809606552, 0.030172914266586304, 0.025221839547157288, 0.03171887993812561, -0.025279255583882332, -0.05385499447584152, -0.017319928854703903, 0.02615794911980629, 0.03106723353266716, -0.018321603536605835, -0.03890589624643326, -0.024325508624315262, 0.03653610870242119, 0.018517831340432167, -0.004494012799113989, 0.060770463198423386, -0.008643320761620998, -0.034977179020643234, -0.04582889750599861, -0.018905285745859146, 0.005996147636324167, -0.01612183451652527, 0.06939784437417984, 0.003852604888379574, -0.0701839029788971, -0.08292588591575623, -0.026484305039048195, -0.08900762349367142, -0.022247757762670517, -0.013534908182919025, 0.0385512076318264, -0.039532650262117386, -0.05805931240320206, 0.019006885588169098, 0.0005042309057898819, -0.06959947198629379, -0.043609146028757095, 0.03509559854865074, 0.05783523619174957, -0.014005331322550774, -0.028270576149225235, 0.043782323598861694, -0.012300968170166016, 0.06069758161902428, 0.09069006890058517, -0.01602812483906746, 0.006933591794222593, -1.5034773426236825e-08, 0.05863410234451294, -0.024170150980353355, -0.026048174127936363, 0.06980574876070023, 0.06045081093907356, 0.006414224859327078, -0.05531464144587517, 0.012504199519753456, 0.04093146324157715, 0.028846364468336105, 0.01760147511959076, 0.005015002563595772, 0.023594560101628304, 0.0516316257417202, 0.0250548105686903, -0.05722063407301903, -0.12157945334911346, -0.00018272407760377973, -0.022113293409347534, -0.08405864983797073, -0.026487981900572777, 0.015580772422254086, -2.8786578695871867e-05, 0.023379845544695854, 0.0061817290261387825, 0.047046516090631485, -0.1017637699842453, 0.07688648253679276, 0.09898848831653595, -0.015477916225790977, 0.008368835784494877, 0.043241195380687714, -0.0442679338157177, 0.02451329305768013, -0.037223149091005325, -0.04206930100917816, -0.01472825463861227, 0.026079634204506874, -0.015963831916451454, 0.1360192447900772, -0.05150851234793663, 0.022148428484797478, 0.027097739279270172, -0.0014352109283208847, -0.06500371545553207, -0.020484279841184616, -0.04887637495994568, 0.0839962512254715, -0.0072325654327869415, -0.020550437271595, -0.06095936894416809, 0.06612943112850189, -0.00011498568346723914, 0.05219540372490883, 0.07801417261362076, 0.017105529084801674, 0.09189657866954803, 0.02236110344529152, -0.07054458558559418, 0.05508328601717949, 0.17235015332698822, -0.004390910267829895, 0.011533830314874649, -0.025606105104088783]\n",
      "Dimension of Embedding:  384\n"
     ]
    }
   ],
   "source": [
    "embedding = embedding_function.embed_documents(\"This is a test sentence.\")\n",
    "\n",
    "print(embedding[0])\n",
    "print(\"Dimension of Embedding: \", len(embedding[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(chunks, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks in DB: 103\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunks in DB:\", db._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Retrieval from Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Literaturverzeichnis', metadata={'page': 11, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf'}),\n",
       " Document(page_content='die Daten zu ziehen und die Daten hoch zu laden. Das Skript konnte in zwei unterschiedlichen Verfahren ausgeführt werden. Das erste Verfahren war ein „Initial load“. Der „Initial load“ wird meistens durchgeführt, wenn noch keine Daten des jeweiligen Kunden vorhanden sind. Das passiert, wenn Kunden', metadata={'page': 7, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf'}),\n",
       " Document(page_content='Einleitung', metadata={'page': 3, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf'}),\n",
       " Document(page_content='zum Studium war teilweise gegeben, da wir im dritten Semester „Wirtschaftsinformatik und angewandte Statistik“ sowie im vierten Semester „Econometrics“ hatten. Im Modul „Wirtschaftsinformatik und angewandte Statistik“ lernten wir die Datenbankabfragesprache SQL sowie Grundlagen des Programmierens', metadata={'page': 9, 'source': '/Users/riccardo/Desktop/Repositorys_Github/LLM/Docs/PR_Ricca.pdf'})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Write a summary of the first page of the document.\"\n",
    "retriever = db.as_retriever()\n",
    "retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Connect to LLM (OpenAI )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key= open_ai_token,\n",
    "    model_name = \"gpt-3.5-turbo\",\n",
    "    temperature = 0.0,\n",
    "    max_tokens = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Connect Vector DB to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",  \n",
    "    retriever = retriever\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 RAG QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'was ist langchain.', 'answer': \"I don't know.\\n\", 'sources': ''}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Welche api haben wir genutzt.\"\n",
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
